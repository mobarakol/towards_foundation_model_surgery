{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "special_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n",
    "_ = tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "questions = [\"hi how are you\"]\n",
    "answers = [\"i am going fine\"]\n",
    "\n",
    "inputs = tokenizer(questions, padding=\"max_length\",max_length= 8, return_tensors=\"pt\")\n",
    "outputs = tokenizer(answers, padding=\"max_length\",max_length= 5, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 5303,   703,   389,   345, 50257, 50257, 50257, 50257]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0]])}\n",
      "{'input_ids': tensor([[   72,   716,  1016,  3734, 50257]]), 'attention_mask': tensor([[1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "questions = [\"hi how are you\"]\n",
    "answers = [\"i am going fine\"]\n",
    "\n",
    "inputs = tokenizer(questions, padding=\"max_length\",max_length= 8, return_tensors=\"pt\")\n",
    "outputs = tokenizer(answers, padding=\"max_length\",max_length= 5, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 5303,   703,   389,   345, 50257, 50257, 50257, 50257]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0]])}\n",
      "{'input_ids': tensor([[50258,    72,   716,  1016,  3734]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "special_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n",
    "_ = tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "questions = [\"hi how are you\"]\n",
    "answers = [\"i am going fine\"]\n",
    "\n",
    "inputs = tokenizer(questions, padding=\"max_length\",max_length= 8, return_tensors=\"pt\")\n",
    "outputs = tokenizer(answers, padding=\"max_length\",max_length= 5, return_tensors=\"pt\")\n",
    "\n",
    "questions = [\"hi how are you\"]\n",
    "answers = [\"<|sep|> i am going fine\"]\n",
    "\n",
    "inputs = tokenizer(questions, padding=\"max_length\",max_length= 8, return_tensors=\"pt\")\n",
    "outputs = tokenizer(answers, padding=\"max_length\",max_length= 5, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config\n",
    "from transformers import GPT2Model, GPT2LMHeadModel\n",
    "\n",
    "## word_embedding\n",
    "# default GPT2 word embedding\n",
    "gpt2configuration = GPT2Config()\n",
    "word_embedder = GPT2Model(gpt2configuration)   \n",
    "word_embedder.resize_token_embeddings(len(tokenizer))\n",
    "word_embedder = word_embedder.wte  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n",
      "tensor([[1, 1, 1, 1, 0, 0, 0, 0]])\n",
      "torch.Size([1, 5, 768])\n",
      "tensor([[1, 1, 1, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "question_embeds = word_embedder(inputs['input_ids'])\n",
    "question_attention_mask = inputs['attention_mask']\n",
    "\n",
    "answer_embeds = word_embedder(outputs['input_ids'])\n",
    "answer_attention_mask = outputs['attention_mask']\n",
    "\n",
    "print(question_embeds.shape)\n",
    "print(question_attention_mask)\n",
    "print(answer_embeds.shape)\n",
    "print(answer_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7]]) 8\n",
      "tensor([[0, 0, 0, 0, 0]])\n",
      "tensor([[0, 1, 2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long)\n",
    "question_position_id = torch.arange(0,question_embeds.size()[1])\n",
    "question_position_id = torch.unsqueeze(question_position_id,0)\n",
    "question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n",
    "# question_position_id = question_position_id.to(device)\n",
    "\n",
    "answer_id_type = torch.zeros(*answer_embeds.size()[:-1], dtype=torch.long)\n",
    "answer_position_id = torch.arange(0,answer_embeds.size()[1])\n",
    "answer_position_id = torch.unsqueeze(answer_position_id,0)\n",
    "answer_position_id = answer_position_id.repeat(answer_embeds.size()[0], 1)\n",
    "# answer_position_id = answer_position_id.to(device)\n",
    "\n",
    "print(question_id_type)\n",
    "print(question_position_id, len(question_position_id[0]))\n",
    "print(answer_id_type)\n",
    "print(answer_position_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8,  9, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "answer_position_id += 8\n",
    "print(answer_position_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embeds = self.visual_embedder(img_feature)\n",
    "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
    "        visual_attention_mask = visual_attention_mask.to(device)\n",
    "\n",
    "        if self.vis_pos_emb == 'zeroes':\n",
    "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            visual_position_id = torch.zeros(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "        elif self.vis_pos_emb == 'pos':\n",
    "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            visual_position_id = torch.arange(0,visual_embeds.size()[1])\n",
    "            visual_position_id = torch.unsqueeze(visual_position_id,0)\n",
    "            visual_position_id = visual_position_id.repeat(visual_embeds.size()[0], 1)\n",
    "            visual_position_id = visual_position_id.to(device)\n",
    "\n",
    "        \n",
    "        ## question embedding:\n",
    "        input['input_ids'] = input['input_ids'].to(device)\n",
    "        input['attention_mask'] = input['attention_mask'].to(device)\n",
    "\n",
    "        \n",
    "        output['input_ids'] = output['input_ids'].to(device)\n",
    "        output['attention_mask'] = output['attention_mask'].to(device)\n",
    "\n",
    "        question_embeds = self.word_embedder(input['input_ids'])\n",
    "        question_attention_mask = input['attention_mask']\n",
    "\n",
    "        answer_embeds = self.word_embedder(output['input_ids'])\n",
    "        answer_attention_mask = output['attention_mask']\n",
    "        \n",
    "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
    "            question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            question_position_id = torch.arange(0,question_embeds.size()[1])\n",
    "            question_position_id = torch.unsqueeze(question_position_id,0)\n",
    "            question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n",
    "            question_position_id = question_position_id.to(device)\n",
    "\n",
    "            answer_id_type = torch.zeros(*answer_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            answer_position_id = torch.arange(0,answer_embeds.size()[1])\n",
    "            answer_position_id = torch.unsqueeze(answer_position_id,0)\n",
    "            answer_position_id = answer_position_id.repeat(answer_embeds.size()[0], 1)\n",
    "            answer_position_id = answer_position_id.to(device)\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        ## combine visual and question embeds\n",
    "        ## vision first\n",
    "        # inputs_embeds = torch.cat((visual_embeds, question_embeds), dim=1)\n",
    "        # attention_mask = torch.cat((visual_attention_mask, question_attention_mask), dim=1)\n",
    "\n",
    "        # if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
    "        #     token_type_ids = torch.cat((visual_id_type, question_id_type), dim=1)\n",
    "        #     position_ids = torch.cat((visual_position_id, question_position_id), dim=1)\n",
    "\n",
    "        ## question first\n",
    "        inputs_embeds = torch.cat((question_embeds, visual_embeds), dim=1)\n",
    "        attention_mask = torch.cat((question_attention_mask, visual_attention_mask), dim=1)\n",
    "\n",
    "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
    "            token_type_ids = torch.cat((question_id_type, visual_id_type), dim=1)\n",
    "            position_ids = torch.cat((question_position_id, visual_position_id), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "from transformers import  VisualBertConfig, GPT2Config\n",
    "from transformers import VisualBertModel, GPT2Model, ViTModel, SwinModel\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "''' Early Fusion GPT with CNN/Transformers'''\n",
    "\n",
    "class EFVLEGPT2RS18Sentence(nn.Module):\n",
    "    def __init__(self, model_subver = 'v3', tokenizer_len=50257, vis_pos_emb = None):\n",
    "        super(EFVLEGPT2RS18Sentence, self).__init__()\n",
    "        '''\n",
    "        v0: visual embedding : Default patch1 + embedding form VB + GPT2 decoder\n",
    "        v1: visual embedding : Default patch1 + from nn.linear    + GPT2 decoder\n",
    "        v2: visual embedding : visual patches + embedding form VB + GPT2 decoder\n",
    "        v3: visual embedding : visual patches + from nn.linear    + GPT2 decoder\n",
    "        '''\n",
    "        \n",
    "        self.sub_ver = model_subver\n",
    "        self.vis_pos_emb = vis_pos_emb\n",
    "        \n",
    "        ## image processing\n",
    "        self.img_feature_extractor = models.resnet18(pretrained=True)\n",
    "        if self.sub_ver == 'v0' or self.sub_ver =='v1':\n",
    "            new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
    "            self.img_feature_extractor.fc = new_fc\n",
    "        elif self.sub_ver == 'v2' or self.sub_ver =='v3':\n",
    "            self.img_feature_extractor = torch.nn.Sequential(*(list(self.img_feature_extractor.children())[:-2]))\n",
    "        \n",
    "        ## Visual_embedding\n",
    "        if self.sub_ver == 'v0' or self.sub_ver =='v2':\n",
    "            # visual bert embedding\n",
    "            VB_config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
    "            VB_config.visual_embedding_dim = 512\n",
    "            visualbert = VisualBertModel(config=VB_config)\n",
    "            self.visual_embedder = visualbert.embeddings.visual_projection\n",
    "        elif self.sub_ver == 'v1' or self.sub_ver =='v3':\n",
    "            self.visual_embedder = nn.Linear(512, 768)\n",
    "\n",
    "        ## word_embedding\n",
    "        # default GPT2 word embedding\n",
    "        gpt2configuration = GPT2Config()\n",
    "        word_embedder = GPT2Model(gpt2configuration)\n",
    "        word_embedder.resize_token_embeddings(tokenizer_len)\n",
    "        self.word_embedder = word_embedder.wte\n",
    "\n",
    "        ## GPT2 visual context aware decoder\n",
    "        self.VCAdecoder = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        \n",
    "        \n",
    "    def forward(self, question, img, answer):\n",
    "        \n",
    "        ## image encoder features\n",
    "        img_feature = self.img_feature_extractor(img)\n",
    "        \n",
    "        if self.sub_ver == 'v0' or self.sub_ver =='v1':\n",
    "            img_feature = torch.unsqueeze(img_feature, dim=1)\n",
    "        if self.sub_ver == 'v2'or self.sub_ver =='v3':\n",
    "            img_feature = torch.flatten(img_feature, start_dim=2)\n",
    "            print(img_feature.shape)\n",
    "            img_feature = img_feature.permute((0,2,1))\n",
    "        \n",
    "        \n",
    "        ## visual Embedding : id type 1, pos: zero / incremental\n",
    "        visual_embeds = self.visual_embedder(img_feature)\n",
    "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
    "        visual_attention_mask = visual_attention_mask.to(device)\n",
    "\n",
    "        if self.vis_pos_emb == 'zeroes':\n",
    "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            visual_position_id = torch.zeros(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "        elif self.vis_pos_emb == 'pos':\n",
    "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            visual_position_id = torch.arange(0,visual_embeds.size()[1])\n",
    "            visual_position_id = torch.unsqueeze(visual_position_id,0)\n",
    "            visual_position_id = visual_position_id.repeat(visual_embeds.size()[0], 1)\n",
    "            visual_position_id = visual_position_id.to(device)\n",
    "        visual_len = len(visual_position_id[0])\n",
    "\n",
    "        \n",
    "        ## question embedding:\n",
    "        question['input_ids'] = question['input_ids'].to(device)\n",
    "        question_embeds = self.word_embedder(question['input_ids'])\n",
    "        question_attention_mask = question['attention_mask'].to(device)\n",
    "\n",
    "        ## answer embedding        \n",
    "        answer['input_ids'] = answer['input_ids'].to(device)        \n",
    "        answer_embeds = self.word_embedder(answer['input_ids'])\n",
    "        answer_attention_mask = answer['attention_mask'].to(device)\n",
    "        \n",
    "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
    "            question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            question_position_id = torch.arange(0,question_embeds.size()[1])\n",
    "            question_position_id = torch.unsqueeze(question_position_id,0)\n",
    "            question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n",
    "            question_position_id = question_position_id.to(device)\n",
    "            question_len = len(question_position_id[0])\n",
    "\n",
    "            answer_id_type = torch.zeros(*answer_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            answer_position_id = torch.arange(0,answer_embeds.size()[1])\n",
    "            answer_position_id = torch.unsqueeze(answer_position_id,0)\n",
    "            answer_position_id = answer_position_id.repeat(answer_embeds.size()[0], 1)\n",
    "            answer_position_id += (question_len+visual_len)\n",
    "            answer_position_id = answer_position_id.to(device)\n",
    "            \n",
    "        print(question_len, visual_len, len(answer_position_id[0]))\n",
    "\n",
    "        ## combine visual and question embeds\n",
    "        ## vision first\n",
    "        # inputs_embeds = torch.cat((visual_embeds, question_embeds), dim=1)\n",
    "        # attention_mask = torch.cat((visual_attention_mask, question_attention_mask), dim=1)\n",
    "\n",
    "        # if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
    "        #     token_type_ids = torch.cat((visual_id_type, question_id_type), dim=1)\n",
    "        #     position_ids = torch.cat((visual_position_id, question_position_id), dim=1)\n",
    "\n",
    "        ## question first\n",
    "        inputs_embeds = torch.cat((question_embeds, visual_embeds, answer_embeds), dim=1)\n",
    "        attention_mask = torch.cat((question_attention_mask, visual_attention_mask, answer_attention_mask), dim=1)\n",
    "\n",
    "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
    "            token_type_ids = torch.cat((question_id_type, visual_id_type, answer_id_type), dim=1)\n",
    "            position_ids = torch.cat((question_position_id, visual_position_id, answer_position_id), dim=1)\n",
    "\n",
    "\n",
    "        ## VCA_GPT2 decoder\n",
    "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
    "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids = position_ids, token_type_ids = token_type_ids)\n",
    "        else:\n",
    "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "special_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n",
    "_ = tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "questions = [\"hi how are you\"]\n",
    "answers = [\"i am going fine\"]\n",
    "\n",
    "inputs = tokenizer(questions, padding=\"max_length\",max_length= 8, return_tensors=\"pt\")\n",
    "outputs = tokenizer(answers, padding=\"max_length\",max_length= 5, return_tensors=\"pt\")\n",
    "image = torch.rand(1,3,300,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len = len(tokenizer)\n",
    "model = EFVLEGPT2RS18Sentence(model_subver = 'v3', tokenizer_len= token_len, vis_pos_emb ='zeroes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 80])\n",
      "8 80 5\n"
     ]
    }
   ],
   "source": [
    "out = model(inputs, image, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 93, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(out[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9027, 0.6617, 0.0132, 0.4579, 0.3671, 0.5998, 0.6740, 0.7606],\n",
      "        [0.5817, 0.6773, 0.6781, 0.1662, 0.7119, 0.9704, 0.9840, 0.0328]])\n",
      "tensor([[0.4579, 0.3671, 0.5998, 0.6740],\n",
      "        [0.1662, 0.7119, 0.9704, 0.9840]])\n",
      "tensor([[0.3671, 0.5998, 0.6740, 0.7606],\n",
      "        [0.7119, 0.9704, 0.9840, 0.0328]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,8)\n",
    "print(a)\n",
    "\n",
    "shift_logits = a[..., 3:-1]\n",
    "shift_labels = a[..., 3+1:]\n",
    "        \n",
    "print(shift_logits)\n",
    "print(shift_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 5303,   703,   389,   345, 50257, 50257, 50257, 50257]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0]])}\n",
      "{'input_ids': tensor([[50258,    72,   716,  1016,  3734]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "special_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n",
    "_ = tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "questions = [\"hi how are you\"]\n",
    "answers = [\"i am going fine\"]\n",
    "\n",
    "inputs = tokenizer(questions, padding=\"max_length\",max_length= 8, return_tensors=\"pt\")\n",
    "outputs = tokenizer(answers, padding=\"max_length\",max_length= 5, return_tensors=\"pt\")\n",
    "\n",
    "questions = [\"hi how are you\"]\n",
    "answers = [\"<|sep|> i am going fine\"]\n",
    "\n",
    "inputs = tokenizer(questions, padding=\"max_length\",max_length= 8, return_tensors=\"pt\")\n",
    "outputs = tokenizer(answers, padding=\"max_length\",max_length= 5, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  72,  716, 1016, 3734]])\n"
     ]
    }
   ],
   "source": [
    "shift_labels = outputs['input_ids'][..., 1:].contiguous()\n",
    "print(shift_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9419, 0.6827, 0.3752, 0.0465],\n",
      "         [0.3533, 0.7009, 0.3063, 0.1070],\n",
      "         [0.1069, 0.2175, 0.2988, 0.5261],\n",
      "         [0.2222, 0.0170, 0.1004, 0.9328]],\n",
      "\n",
      "        [[0.2506, 0.2457, 0.4110, 0.8580],\n",
      "         [0.8879, 0.3518, 0.6792, 0.4434],\n",
      "         [0.8882, 0.4520, 0.4354, 0.1541],\n",
      "         [0.5476, 0.1973, 0.4182, 0.2564]]])\n",
      "tensor([[0, 1, 3, 3],\n",
      "        [3, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,4,4)\n",
    "print(a)\n",
    "_, p_id = torch.max(a, dim=2)\n",
    "print(p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi how are you', 'i am doing fine']\n",
      "['hi how are you', 'i am doing fine']\n"
     ]
    }
   ],
   "source": [
    "answers_GT = [\"<|sep|> hi how are you\", \"<|sep|> i am doing fine\"]\n",
    "answers_Gen = [\"hi how are you\", \"i am doing fine\"]\n",
    "\n",
    "labels = tokenizer(answers_GT, padding=\"max_length\",max_length= 8, return_tensors=\"pt\")\n",
    "logits = tokenizer(answers_Gen, padding=\"max_length\",max_length= 5, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "shift_labels = labels['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
    "logits = labels['input_ids'].contiguous() # 1 because answer has '<|sep|>' in front\n",
    "\n",
    "\n",
    "answer_GT_dec = tokenizer.batch_decode(shift_labels, skip_special_tokens= True)\n",
    "print(answer_GT_dec)\n",
    "answer_Gen_dec = tokenizer.batch_decode(logits, skip_special_tokens= True)\n",
    "print(answer_GT_dec)\n",
    "\n",
    "references = []\n",
    "hypotheses = []\n",
    "\n",
    "for answer_GT_dec_i in answer_GT_dec: references.append([answer_GT_dec_i.split()])\n",
    "print(references)\n",
    "\n",
    "# Hypotheses\n",
    "for answer_Gen_dec_i in answer_Gen_dec: hypotheses.append(answer_Gen_dec_i.split())\n",
    "print(hypotheses)\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Calculate BLEU1~4\n",
    "metrics = {}\n",
    "metrics[\"Bleu_1\"] = corpus_bleu(references, hypotheses, weights=(1.00, 0.00, 0.00, 0.00))\n",
    "metrics[\"Bleu_2\"] = corpus_bleu(references, hypotheses, weights=(0.50, 0.50, 0.00, 0.00))\n",
    "metrics[\"Bleu_3\"] = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0.00))\n",
    "metrics[\"Bleu_4\"] = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "print(\"BLEU-1 {:.6f} BLEU2 {:.6f} BLEU3 {:.6f} BLEU-4 {:.6f}\".format\n",
    "          (metrics[\"Bleu_1\"],  metrics[\"Bleu_2\"],  metrics[\"Bleu_3\"],  metrics[\"Bleu_4\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['hi', 'how', 'are', 'you']], [['i', 'am', 'doing', 'fine']]]\n",
      "[['hi', 'how', 'are', 'you'], ['i', 'am', 'doing', 'fine']]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-591aa5cabd87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleu_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpus_bleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Calculate BLEU1~4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Bleu_1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotheses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.00\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Calculate BLEU1~4\n",
    "metrics = {}\n",
    "metrics[\"Bleu_1\"] = corpus_bleu(references, hypotheses, weights=(1.00, 0.00, 0.00, 0.00))\n",
    "metrics[\"Bleu_2\"] = corpus_bleu(references, hypotheses, weights=(0.50, 0.50, 0.00, 0.00))\n",
    "metrics[\"Bleu_3\"] = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0.00))\n",
    "metrics[\"Bleu_4\"] = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "print(\"BLEU-1 {:.6f} BLEU2 {:.6f} BLEU3 {:.6f} BLEU-4 {:.6f}\".format\n",
    "          (metrics[\"Bleu_1\"],  metrics[\"Bleu_2\"],  metrics[\"Bleu_3\"],  metrics[\"Bleu_4\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
