{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "special_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n",
    "_ = tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "questions = [\"hi how are you\"]\n",
    "answers = [\"i am going fine\"]\n",
    "\n",
    "inputs = tokenizer(questions, padding=\"max_length\",max_length= 8, return_tensors=\"pt\")\n",
    "outputs = tokenizer(answers, padding=\"max_length\",max_length= 5, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 5303,   703,   389,   345, 50257, 50257, 50257, 50257]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0]])}\n",
      "{'input_ids': tensor([[   72,   716,  1016,  3734, 50257]]), 'attention_mask': tensor([[1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "questions = [\"hi how are you\"]\n",
    "answers = [\"i am going fine\"]\n",
    "\n",
    "inputs = tokenizer(questions, padding=\"max_length\",max_length= 8, return_tensors=\"pt\")\n",
    "outputs = tokenizer(answers, padding=\"max_length\",max_length= 5, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 5303,   703,   389,   345, 50257, 50257, 50257, 50257]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0]])}\n",
      "{'input_ids': tensor([[50258,    72,   716,  1016,  3734]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "special_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n",
    "_ = tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "questions = [\"hi how are you\"]\n",
    "answers = [\"i am going fine\"]\n",
    "\n",
    "inputs = tokenizer(questions, padding=\"max_length\",max_length= 8, return_tensors=\"pt\")\n",
    "outputs = tokenizer(answers, padding=\"max_length\",max_length= 5, return_tensors=\"pt\")\n",
    "\n",
    "questions = [\"hi how are you\"]\n",
    "answers = [\"<|sep|> i am going fine\"]\n",
    "\n",
    "inputs = tokenizer(questions, padding=\"max_length\",max_length= 8, return_tensors=\"pt\")\n",
    "outputs = tokenizer(answers, padding=\"max_length\",max_length= 5, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config\n",
    "from transformers import GPT2Model, GPT2LMHeadModel\n",
    "\n",
    "## word_embedding\n",
    "# default GPT2 word embedding\n",
    "gpt2configuration = GPT2Config()\n",
    "word_embedder = GPT2Model(gpt2configuration)   \n",
    "word_embedder.resize_token_embeddings(len(tokenizer))\n",
    "word_embedder = word_embedder.wte  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n",
      "tensor([[1, 1, 1, 1, 0, 0, 0, 0]])\n",
      "torch.Size([1, 5, 768])\n",
      "tensor([[1, 1, 1, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "question_embeds = word_embedder(inputs['input_ids'])\n",
    "question_attention_mask = inputs['attention_mask']\n",
    "\n",
    "answer_embeds = word_embedder(outputs['input_ids'])\n",
    "answer_attention_mask = outputs['attention_mask']\n",
    "\n",
    "print(question_embeds.shape)\n",
    "print(question_attention_mask)\n",
    "print(answer_embeds.shape)\n",
    "print(answer_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7]]) 8\n",
      "tensor([[0, 0, 0, 0, 0]])\n",
      "tensor([[0, 1, 2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long)\n",
    "question_position_id = torch.arange(0,question_embeds.size()[1])\n",
    "question_position_id = torch.unsqueeze(question_position_id,0)\n",
    "question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n",
    "# question_position_id = question_position_id.to(device)\n",
    "\n",
    "answer_id_type = torch.zeros(*answer_embeds.size()[:-1], dtype=torch.long)\n",
    "answer_position_id = torch.arange(0,answer_embeds.size()[1])\n",
    "answer_position_id = torch.unsqueeze(answer_position_id,0)\n",
    "answer_position_id = answer_position_id.repeat(answer_embeds.size()[0], 1)\n",
    "# answer_position_id = answer_position_id.to(device)\n",
    "\n",
    "print(question_id_type)\n",
    "print(question_position_id, len(question_position_id[0]))\n",
    "print(answer_id_type)\n",
    "print(answer_position_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8,  9, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "answer_position_id += 8\n",
    "print(answer_position_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embeds = self.visual_embedder(img_feature)\n",
    "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
    "        visual_attention_mask = visual_attention_mask.to(device)\n",
    "\n",
    "        if self.vis_pos_emb == 'zeroes':\n",
    "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            visual_position_id = torch.zeros(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "        elif self.vis_pos_emb == 'pos':\n",
    "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            visual_position_id = torch.arange(0,visual_embeds.size()[1])\n",
    "            visual_position_id = torch.unsqueeze(visual_position_id,0)\n",
    "            visual_position_id = visual_position_id.repeat(visual_embeds.size()[0], 1)\n",
    "            visual_position_id = visual_position_id.to(device)\n",
    "\n",
    "        \n",
    "        ## question embedding:\n",
    "        input['input_ids'] = input['input_ids'].to(device)\n",
    "        input['attention_mask'] = input['attention_mask'].to(device)\n",
    "\n",
    "        \n",
    "        output['input_ids'] = output['input_ids'].to(device)\n",
    "        output['attention_mask'] = output['attention_mask'].to(device)\n",
    "\n",
    "        question_embeds = self.word_embedder(input['input_ids'])\n",
    "        question_attention_mask = input['attention_mask']\n",
    "\n",
    "        answer_embeds = self.word_embedder(output['input_ids'])\n",
    "        answer_attention_mask = output['attention_mask']\n",
    "        \n",
    "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
    "            question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            question_position_id = torch.arange(0,question_embeds.size()[1])\n",
    "            question_position_id = torch.unsqueeze(question_position_id,0)\n",
    "            question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n",
    "            question_position_id = question_position_id.to(device)\n",
    "\n",
    "            answer_id_type = torch.zeros(*answer_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            answer_position_id = torch.arange(0,answer_embeds.size()[1])\n",
    "            answer_position_id = torch.unsqueeze(answer_position_id,0)\n",
    "            answer_position_id = answer_position_id.repeat(answer_embeds.size()[0], 1)\n",
    "            answer_position_id = answer_position_id.to(device)\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        ## combine visual and question embeds\n",
    "        ## vision first\n",
    "        # inputs_embeds = torch.cat((visual_embeds, question_embeds), dim=1)\n",
    "        # attention_mask = torch.cat((visual_attention_mask, question_attention_mask), dim=1)\n",
    "\n",
    "        # if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
    "        #     token_type_ids = torch.cat((visual_id_type, question_id_type), dim=1)\n",
    "        #     position_ids = torch.cat((visual_position_id, question_position_id), dim=1)\n",
    "\n",
    "        ## question first\n",
    "        inputs_embeds = torch.cat((question_embeds, visual_embeds), dim=1)\n",
    "        attention_mask = torch.cat((question_attention_mask, visual_attention_mask), dim=1)\n",
    "\n",
    "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
    "            token_type_ids = torch.cat((question_id_type, visual_id_type), dim=1)\n",
    "            position_ids = torch.cat((question_position_id, visual_position_id), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "from transformers import  VisualBertConfig, GPT2Config\n",
    "from transformers import VisualBertModel, GPT2Model, ViTModel, SwinModel\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "''' Early Fusion GPT with CNN/Transformers'''\n",
    "\n",
    "class EFVLEGPT2RS18Sentence(nn.Module):\n",
    "    def __init__(self, model_subver = 'v3', tokenizer_len=50257, vis_pos_emb = None):\n",
    "        super(EFVLEGPT2RS18Sentence, self).__init__()\n",
    "        '''\n",
    "        v0: visual embedding : Default patch1 + embedding form VB + GPT2 decoder\n",
    "        v1: visual embedding : Default patch1 + from nn.linear    + GPT2 decoder\n",
    "        v2: visual embedding : visual patches + embedding form VB + GPT2 decoder\n",
    "        v3: visual embedding : visual patches + from nn.linear    + GPT2 decoder\n",
    "        '''\n",
    "        \n",
    "        self.sub_ver = model_subver\n",
    "        self.vis_pos_emb = vis_pos_emb\n",
    "        \n",
    "        ## image processing\n",
    "        self.img_feature_extractor = models.resnet18(pretrained=True)\n",
    "        if self.sub_ver == 'v0' or self.sub_ver =='v1':\n",
    "            new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
    "            self.img_feature_extractor.fc = new_fc\n",
    "        elif self.sub_ver == 'v2' or self.sub_ver =='v3':\n",
    "            self.img_feature_extractor = torch.nn.Sequential(*(list(self.img_feature_extractor.children())[:-2]))\n",
    "        \n",
    "        ## Visual_embedding\n",
    "        if self.sub_ver == 'v0' or self.sub_ver =='v2':\n",
    "            # visual bert embedding\n",
    "            VB_config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
    "            VB_config.visual_embedding_dim = 512\n",
    "            visualbert = VisualBertModel(config=VB_config)\n",
    "            self.visual_embedder = visualbert.embeddings.visual_projection\n",
    "        elif self.sub_ver == 'v1' or self.sub_ver =='v3':\n",
    "            self.visual_embedder = nn.Linear(512, 768)\n",
    "\n",
    "        ## word_embedding\n",
    "        # default GPT2 word embedding\n",
    "        gpt2configuration = GPT2Config()\n",
    "        word_embedder = GPT2Model(gpt2configuration)\n",
    "        word_embedder.resize_token_embeddings(tokenizer_len)\n",
    "        self.word_embedder = word_embedder.wte\n",
    "\n",
    "        ## GPT2 visual context aware decoder\n",
    "        self.VCAdecoder = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        \n",
    "        \n",
    "    def forward(self, question, img, answer):\n",
    "        \n",
    "        ## image encoder features\n",
    "        img_feature = self.img_feature_extractor(img)\n",
    "        \n",
    "        if self.sub_ver == 'v0' or self.sub_ver =='v1':\n",
    "            img_feature = torch.unsqueeze(img_feature, dim=1)\n",
    "        if self.sub_ver == 'v2'or self.sub_ver =='v3':\n",
    "            img_feature = torch.flatten(img_feature, start_dim=2)\n",
    "            print(img_feature.shape)\n",
    "            img_feature = img_feature.permute((0,2,1))\n",
    "        \n",
    "        \n",
    "        ## visual Embedding : id type 1, pos: zero / incremental\n",
    "        visual_embeds = self.visual_embedder(img_feature)\n",
    "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
    "        visual_attention_mask = visual_attention_mask.to(device)\n",
    "\n",
    "        if self.vis_pos_emb == 'zeroes':\n",
    "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            visual_position_id = torch.zeros(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "        elif self.vis_pos_emb == 'pos':\n",
    "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            visual_position_id = torch.arange(0,visual_embeds.size()[1])\n",
    "            visual_position_id = torch.unsqueeze(visual_position_id,0)\n",
    "            visual_position_id = visual_position_id.repeat(visual_embeds.size()[0], 1)\n",
    "            visual_position_id = visual_position_id.to(device)\n",
    "        visual_len = len(visual_position_id[0])\n",
    "\n",
    "        \n",
    "        ## question embedding:\n",
    "        question['input_ids'] = question['input_ids'].to(device)\n",
    "        question_embeds = self.word_embedder(question['input_ids'])\n",
    "        question_attention_mask = question['attention_mask'].to(device)\n",
    "\n",
    "        ## answer embedding        \n",
    "        answer['input_ids'] = answer['input_ids'].to(device)        \n",
    "        answer_embeds = self.word_embedder(answer['input_ids'])\n",
    "        answer_attention_mask = answer['attention_mask'].to(device)\n",
    "        \n",
    "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
    "            question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            question_position_id = torch.arange(0,question_embeds.size()[1])\n",
    "            question_position_id = torch.unsqueeze(question_position_id,0)\n",
    "            question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n",
    "            question_position_id = question_position_id.to(device)\n",
    "            question_len = len(question_position_id[0])\n",
    "\n",
    "            answer_id_type = torch.zeros(*answer_embeds.size()[:-1], dtype=torch.long, device=device)\n",
    "            answer_position_id = torch.arange(0,answer_embeds.size()[1])\n",
    "            answer_position_id = torch.unsqueeze(answer_position_id,0)\n",
    "            answer_position_id = answer_position_id.repeat(answer_embeds.size()[0], 1)\n",
    "            answer_position_id += (question_len+visual_len)\n",
    "            answer_position_id = answer_position_id.to(device)\n",
    "            \n",
    "        print(question_len, visual_len, len(answer_position_id[0]))\n",
    "\n",
    "        ## combine visual and question embeds\n",
    "        ## vision first\n",
    "        # inputs_embeds = torch.cat((visual_embeds, question_embeds), dim=1)\n",
    "        # attention_mask = torch.cat((visual_attention_mask, question_attention_mask), dim=1)\n",
    "\n",
    "        # if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
    "        #     token_type_ids = torch.cat((visual_id_type, question_id_type), dim=1)\n",
    "        #     position_ids = torch.cat((visual_position_id, question_position_id), dim=1)\n",
    "\n",
    "        ## question first\n",
    "        inputs_embeds = torch.cat((question_embeds, visual_embeds, answer_embeds), dim=1)\n",
    "        attention_mask = torch.cat((question_attention_mask, visual_attention_mask, answer_attention_mask), dim=1)\n",
    "\n",
    "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
    "            token_type_ids = torch.cat((question_id_type, visual_id_type, answer_id_type), dim=1)\n",
    "            position_ids = torch.cat((question_position_id, visual_position_id, answer_position_id), dim=1)\n",
    "\n",
    "\n",
    "        ## VCA_GPT2 decoder\n",
    "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
    "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids = position_ids, token_type_ids = token_type_ids)\n",
    "        else:\n",
    "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "special_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n",
    "_ = tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "questions = [\"hi how are you\"]\n",
    "answers = [\"i am going fine\"]\n",
    "\n",
    "inputs = tokenizer(questions, padding=\"max_length\",max_length= 8, return_tensors=\"pt\")\n",
    "outputs = tokenizer(answers, padding=\"max_length\",max_length= 5, return_tensors=\"pt\")\n",
    "image = torch.rand(1,3,300,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len = len(tokenizer)\n",
    "model = EFVLEGPT2RS18Sentence(model_subver = 'v3', tokenizer_len= token_len, vis_pos_emb ='zeroes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 80])\n",
      "8 80 5\n"
     ]
    }
   ],
   "source": [
    "out = model(inputs, image, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 93, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(out[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9027, 0.6617, 0.0132, 0.4579, 0.3671, 0.5998, 0.6740, 0.7606],\n",
      "        [0.5817, 0.6773, 0.6781, 0.1662, 0.7119, 0.9704, 0.9840, 0.0328]])\n",
      "tensor([[0.4579, 0.3671, 0.5998, 0.6740],\n",
      "        [0.1662, 0.7119, 0.9704, 0.9840]])\n",
      "tensor([[0.3671, 0.5998, 0.6740, 0.7606],\n",
      "        [0.7119, 0.9704, 0.9840, 0.0328]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,8)\n",
    "print(a)\n",
    "\n",
    "shift_logits = a[..., 3:-1]\n",
    "shift_labels = a[..., 3+1:]\n",
    "        \n",
    "print(shift_logits)\n",
    "print(shift_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
